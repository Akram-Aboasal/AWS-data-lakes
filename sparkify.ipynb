{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee5b2a-d023-44de-99f5-005b7a95f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5403d-a803-4894-8b8f-9c84fa475f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb87b3-9f8b-41f7-b126-0c477e23e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e161e-1b51-4f16-8f64-a0be10a7cfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\" Reading song data and create songs and artists table\n",
    "    \n",
    "        Arguments:\n",
    "            spark {object}: SparkSession object\n",
    "            input_data {object}: Source S3 endpoint\n",
    "            output_data {object}: Target S3 endpoint\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + \"song_data/*/*/*/\"\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "    df.count()\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select([\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"]).distinct()\n",
    "    print(songs_table.show(5, False))\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    " \n",
    "    songs_table.write.mode(\"overwrite\").parquet(output_data+'songs/'+'songs.parquet', partitionBy=['year','artist_id'])\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select([\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\"]).distinct() \n",
    "    print(artists_table.show(5, truncate = False))\n",
    "    # write artists table to parquet files\n",
    " \n",
    "    artists_table.write.mode(\"overwrite\").parquet(output_data + 'artists/' + 'artists.parquet', partitionBy=['artist_id'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed8e1d-83ed-4ed4-a60b-0be6c6a6afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\" Reading log data and create songs and artists table\n",
    "    \n",
    "        Arguments:\n",
    "            spark {object}: SparkSession object\n",
    "            input_data {object}: Source S3 endpoint\n",
    "            output_data {object}: Target S3 endpoint\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # get filepath to log data file\n",
    "    log_data =input_data + \"log_data/\"\n",
    "\n",
    "    # read log data file\n",
    "    log_df = spark.read.json(log_data)\n",
    "    print(log_df.show(2))\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    log_df = log_df.where(log_df['page'] == 'NextSong')\n",
    "    print(log_df.show(2))\n",
    "    \n",
    "    # extract columns for users table    \n",
    "    users_table = log_df.select('userId', 'firstName', 'lastName', 'gender', 'level').distinct()\n",
    "    print(users_table.show(5, truncate = False))\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.mode(\"overwrite\").parquet(output_data + 'users/' + 'users.parquet', partitionBy = ['userId'])\n",
    "    \n",
    "    # create timestamp column from original timestamp column\n",
    "    log_df = log_df.withColumn('timestamp',( (log_df.ts.cast('float')/1000).cast(\"timestamp\")))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = log_df.select(\n",
    "                    F.col(\"timestamp\").alias(\"start_time\"),\n",
    "                    F.hour(\"timestamp\").alias('hour'),\n",
    "                    F.dayofmonth(\"timestamp\").alias('day'),\n",
    "                    F.weekofyear(\"timestamp\").alias('week'),\n",
    "                    F.month(\"timestamp\").alias('month'), \n",
    "                    F.year(\"timestamp\").alias('year'), \n",
    "                    F.date_format(F.col(\"timestamp\"), \"E\").alias(\"weekday\")\n",
    "                )\n",
    "\n",
    "\n",
    "    time_table.show(5, False)\n",
    "\n",
    "    # write time table to parquet files partitioned by year and month\n",
    "   \n",
    "    time_table.write.mode(\"overwrite\").parquet(output_data + 'time/' + 'time.parquet', partitionBy=['year','month'])\n",
    "\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read.json(input_data + \"song_data/*/*/*/\")\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = log_df.join(song_df, (log_df.song == song_df.title) & (log_df.artist == song_df.artist_name) & (log_df.length == song_df.duration), how='inner')\n",
    "    songplays_table = songplays_table.distinct() \\\n",
    "                        .select(\"userId\", \"timestamp\", \"song_id\", \"artist_id\", \"level\", \"sessionId\", \"location\", \"userAgent\" ) \\\n",
    "                        .withColumn(\"songplay_id\", F.row_number().over( Window.partitionBy('timestamp').orderBy(\"timestamp\"))) \\\n",
    "                        .withColumnRenamed(\"userId\",\"user_id\")        \\\n",
    "                        .withColumnRenamed(\"timestamp\",\"start_time\")  \\\n",
    "                        .withColumnRenamed(\"sessionId\",\"session_id\")  \\\n",
    "                        .withColumnRenamed(\"userAgent\", \"user_agent\") \\\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    print(songplays_table.show(5))\n",
    "    songplays_table.write.mode(\"overwrite\").parquet(output_data + 'songplays/' + 'songplays.parquet',partitionBy=['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc04b9d6-7793-4b6e-b0c7-eeab307a5786",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73932194-49b8-445f-8f22-cd38036abf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend-dl-lake/\"\n",
    "output_data = \"s3a://sparkify-udacity-data-lake/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b4f4b-15a0-48a5-90e8-7c853183f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_song_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56532e56-2fb4-4d93-96d7-21095ac1c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bdc5c8-14f3-4df0-8fcb-8ea62d9d3982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
