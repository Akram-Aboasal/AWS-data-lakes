{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\" Reading song data and create songs and artists table\n",
    "    \n",
    "        Arguments:\n",
    "            spark {object}: SparkSession object\n",
    "            input_data {object}: Source S3 endpoint\n",
    "            output_data {object}: Target S3 endpoint\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + \"song_data/*/*/*/\"\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "    df.count()\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select([\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"]).distinct()\n",
    "    print(songs_table.show(5, False))\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    " \n",
    "    songs_table.write.mode(\"overwrite\").parquet(output_data+'songs/'+'songs.parquet', partitionBy=['year','artist_id'])\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select([\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\"]).distinct() \n",
    "    print(artists_table.show(5, truncate = False))\n",
    "    # write artists table to parquet files\n",
    " \n",
    "    artists_table.write.mode(\"overwrite\").parquet(output_data + 'artists/' + 'artists.parquet', partitionBy=['artist_id'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\" Reading log data and create songs and artists table\n",
    "    \n",
    "        Arguments:\n",
    "            spark {object}: SparkSession object\n",
    "            input_data {object}: Source S3 endpoint\n",
    "            output_data {object}: Target S3 endpoint\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # get filepath to log data file\n",
    "\n",
    "    log_data = input_data + \"log-data/\"\n",
    "\n",
    "    # read log data file\n",
    "    log_df = spark.read.json(log_data)\n",
    "    print(log_df.show(2))\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    log_df = log_df.where(log_df['page'] == 'NextSong')\n",
    "    print(log_df.show(2))\n",
    "    \n",
    "    # extract columns for users table    \n",
    "    users_table = log_df.select('userId', 'firstName', 'lastName', 'gender', 'level').distinct()\n",
    "    print(users_table.show(5, truncate = False))\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.mode(\"overwrite\").parquet(output_data + 'users/' + 'users.parquet', partitionBy = ['userId'])\n",
    "    \n",
    "    # create timestamp column from original timestamp column\n",
    "    log_df = log_df.withColumn('timestamp',( (log_df.ts.cast('float')/1000).cast(\"timestamp\")))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = log_df.select(\n",
    "                    F.col(\"timestamp\").alias(\"start_time\"),\n",
    "                    F.hour(\"timestamp\").alias('hour'),\n",
    "                    F.dayofmonth(\"timestamp\").alias('day'),\n",
    "                    F.weekofyear(\"timestamp\").alias('week'),\n",
    "                    F.month(\"timestamp\").alias('month'), \n",
    "                    F.year(\"timestamp\").alias('year'), \n",
    "                    F.date_format(F.col(\"timestamp\"), \"E\").alias(\"weekday\")\n",
    "                )\n",
    "\n",
    "\n",
    "    time_table.show(5, False)\n",
    "\n",
    "    # write time table to parquet files partitioned by year and month\n",
    "   \n",
    "    time_table.write.mode(\"overwrite\").parquet(output_data + 'time/' + 'time.parquet', partitionBy=['year','month'])\n",
    "\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read.json(input_data + \"song_data/*/*/*/\")\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = log_df.join(song_df, (log_df.song == song_df.title) & (log_df.artist == song_df.artist_name) & (log_df.length == song_df.duration), how='inner')\n",
    "    songplays_table = songplays_table.distinct() \\\n",
    "                        .select(\"userId\", \"timestamp\", \"song_id\", \"artist_id\", \"level\", \"sessionId\", \"location\", \"userAgent\" ) \\\n",
    "                        .withColumn(\"songplay_id\", F.row_number().over( Window.partitionBy('timestamp').orderBy(\"timestamp\"))) \\\n",
    "                        .withColumnRenamed(\"userId\",\"user_id\")        \\\n",
    "                        .withColumnRenamed(\"timestamp\",\"start_time\")  \\\n",
    "                        .withColumnRenamed(\"sessionId\",\"session_id\")  \\\n",
    "                        .withColumnRenamed(\"userAgent\", \"user_agent\") \\\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    print(songplays_table.show(5))\n",
    "    songplays_table.write.mode(\"overwrite\").parquet(output_data + 'songplays/' + 'songplays.parquet',partitionBy=['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-sparkify-dl/\"\n",
    "output_data = \"s3a://udacity-sparkify-dl/output_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------------------------------------+------------------+----+---------+\n",
      "|song_id           |title                                              |artist_id         |year|duration |\n",
      "+------------------+---------------------------------------------------+------------------+----+---------+\n",
      "|SOGOSOV12AF72A285E|¿Dónde va Chichi?                                  |ARGUVEV1187B98BA17|1997|313.12934|\n",
      "|SOTTDKS12AB018D69B|It Wont Be Christmas                               |ARMBR4Y1187B9990EB|0   |241.47546|\n",
      "|SOBBUGU12A8C13E95D|Setting Fire to Sleeping Giants                    |ARMAC4T1187FB3FA4C|2004|207.77751|\n",
      "|SOIAZJW12AB01853F1|Pink World                                         |AR8ZCNI1187B9A069B|1984|269.81832|\n",
      "|SONYPOM12A8C13B2D7|I Think My Wife Is Running Around On Me (Taco Hell)|ARDNS031187B9924F0|2005|186.48771|\n",
      "+------------------+---------------------------------------------------+------------------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "+------------------+---------------+---------------+---------------+----------------+\n",
      "|artist_id         |artist_name    |artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+---------------+---------------+---------------+----------------+\n",
      "|AR3JMC51187B9AE49D|Backstreet Boys|Orlando, FL    |28.53823       |-81.37739       |\n",
      "|AR0IAWL1187B9A96D0|Danilo Perez   |Panama         |8.4177         |-80.11278       |\n",
      "|ARWB3G61187FB49404|Steve Morse    |Hamilton, Ohio |null           |null            |\n",
      "|AR47JEX1187B995D81|SUE THOMPSON   |Nevada, MO     |37.83721       |-94.35868       |\n",
      "|ARHHO3O1187B989413|Bob Azzam      |               |null           |null            |\n",
      "+------------------+---------------+---------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "process_song_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
